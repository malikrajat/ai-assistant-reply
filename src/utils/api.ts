import {
  UserSettings,
  GeminiRequest,
  GeminiResponse,
} from "../types/index.js";

/**
 * Gemini API endpoint
 */
const GEMINI_API_BASE = "https://generativelanguage.googleapis.com/v1beta";
const GEMINI_MODEL = "gemini-2.5-flash";

/**
 * Builds a prompt for the Gemini API based on post text and user settings
 */
export function buildPrompt(
  postText: string,
  settings: UserSettings
): string {
  const toneInstructions: Record<UserSettings["tone"], string> = {
    polite: "Write a polite and respectful reply",
    professional: "Write a professional and business-appropriate reply",
    friendly: "Write a friendly and warm reply",
    concise: "Write a brief and to-the-point reply",
  };

  const instruction = toneInstructions[settings.tone];

  return `${instruction} to the following LinkedIn post. 
Keep it under ${settings.maxLength} characters. 
Do not include greetings or signatures.

Post:
${postText}

Reply:`;
}

/**
 * Maps tone settings to API temperature values
 * Higher temperature = more creative/varied responses
 */
export function getToneTemperature(tone: UserSettings["tone"]): number {
  const temperatureMap: Record<UserSettings["tone"], number> = {
    polite: 0.7,
    professional: 0.5,
    friendly: 0.8,
    concise: 0.3,
  };

  return temperatureMap[tone];
}

/**
 * Extracts reply text from Gemini API response
 */
export function extractReplyText(response: GeminiResponse): string {
  // Log the full response for debugging
  console.log("[API] Full Gemini response:", JSON.stringify(response, null, 2));

  // Check for API error
  if (response.error) {
    throw new Error(
      `Gemini API error: ${response.error.message} (${response.error.status})`
    );
  }

  // Check for candidates
  if (!response.candidates || response.candidates.length === 0) {
    console.error("[API] No candidates in response:", response);
    throw new Error("No reply generated by Gemini API");
  }

  // Extract text from first candidate
  const candidate = response.candidates[0];
  console.log("[API] First candidate:", JSON.stringify(candidate, null, 2));

  // Check for MAX_TOKENS finish reason
  if (candidate.finishReason === "MAX_TOKENS") {
    throw new Error("Response was cut off due to token limit. Please reduce the max length setting or try a shorter post.");
  }

  if (!candidate.content) {
    console.error("[API] No content in candidate:", candidate);
    throw new Error("Invalid response format from Gemini API: missing content");
  }

  if (!candidate.content.parts || candidate.content.parts.length === 0) {
    console.error("[API] No parts in content:", candidate.content);
    throw new Error("Invalid response format from Gemini API: missing parts");
  }

  const replyText = candidate.content.parts[0].text;

  if (!replyText || replyText.trim().length === 0) {
    console.error("[API] Empty text in part:", candidate.content.parts[0]);
    throw new Error("Empty reply generated by Gemini API");
  }

  return replyText.trim();
}

/**
 * Fetch with retry logic and exponential backoff
 */
export async function fetchWithRetry(
  url: string,
  options: RequestInit,
  maxRetries = 3
): Promise<Response> {
  let lastError: Error | null = null;

  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await fetch(url, options);

      // Success - return response
      if (response.ok) {
        return response;
      }

      // Don't retry on client errors (4xx) - these won't succeed on retry
      if (response.status >= 400 && response.status < 500) {
        const errorText = await response.text();
        throw new Error(
          `API error ${response.status}: ${errorText || response.statusText}`
        );
      }

      // Server error (5xx) - will retry
      lastError = new Error(
        `Server error ${response.status}: ${response.statusText}`
      );
    } catch (error) {
      lastError = error as Error;

      // If this is the last retry, throw the error
      if (i === maxRetries - 1) {
        throw lastError;
      }

      // Exponential backoff: wait 1s, 2s, 4s...
      const waitTime = Math.pow(2, i) * 1000;
      console.log(
        `[API] Retry ${i + 1}/${maxRetries} after ${waitTime}ms...`
      );
      await sleep(waitTime);
    }
  }

  // Should never reach here, but TypeScript needs it
  throw lastError || new Error("Unknown error during fetch");
}

/**
 * Sleep utility for retry delays
 */
function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

/**
 * Calls the Gemini API to generate a reply
 */
export async function callGeminiAPI(
  postText: string,
  settings: UserSettings
): Promise<string> {
  // Validate API key
  if (!settings.apiKey || settings.apiKey.trim().length === 0) {
    throw new Error("API key not configured");
  }

  // Build the prompt
  const prompt = buildPrompt(postText, settings);

  // Construct API endpoint
  const endpoint = `${GEMINI_API_BASE}/models/${GEMINI_MODEL}:generateContent?key=${settings.apiKey}`;

  // Build request body
  // Note: Gemini 2.5 Flash uses "thinking tokens" internally which count against the limit
  // We need to allocate MUCH more tokens for the model's internal reasoning
  // The model can use 100-200+ tokens just for thinking before generating output
  const outputTokens = Math.ceil((settings.maxLength || 500) / 2);
  const thinkingTokens = 500; // Large buffer for model's internal reasoning (increased from 200)
  const maxTokens = outputTokens + thinkingTokens;

  const requestBody: GeminiRequest = {
    contents: [
      {
        parts: [{ text: prompt }],
      },
    ],
    generationConfig: {
      maxOutputTokens: Math.max(maxTokens, 1024), // Minimum 1024 tokens to ensure enough for thinking + output
      temperature: getToneTemperature(settings.tone),
    },
  };

  try {
    // Make API call with retry logic
    const response = await fetchWithRetry(
      endpoint,
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      },
      3 // max retries
    );

    // Parse response
    const data: GeminiResponse = await response.json();

    // Extract and return reply text
    return extractReplyText(data);
  } catch (error) {
    console.error("[API] Error calling Gemini API:", error);

    // Provide user-friendly error messages
    if (error instanceof Error) {
      if (error.message.includes("API key")) {
        throw new Error("Invalid API key. Please check your settings.");
      }
      if (error.message.includes("quota")) {
        throw new Error("API quota exceeded. Please try again later.");
      }
      if (error.message.includes("Network") || error.message.includes("fetch")) {
        throw new Error("Network error. Please check your connection.");
      }
    }

    throw error;
  }
}

/**
 * Sanitizes post text before sending to API
 * Removes excessive whitespace and limits length
 */
export function sanitizePostText(text: string, maxLength = 5000): string {
  // Remove excessive whitespace
  let sanitized = text.trim().replace(/\s+/g, " ");

  // Limit length to prevent API issues
  if (sanitized.length > maxLength) {
    sanitized = sanitized.substring(0, maxLength) + "...";
  }

  return sanitized;
}
